---
title: "Multiparametric Imaging: CODEX"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Multiparametric Imaging: CODEX}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# Multiparametric Imaging: CODEX

```{r setup}
library(pathml)
```

```bash
rsync -e ssh -azvhPr TMA_A omega:workspaces/temp/pathml/data/CRC
rsync -e ssh -azvhPr TMA_B omega:workspaces/temp/pathml/data/CRC
```

```python
from os import listdir,path,getcwd
import glob
import re
import pandas as pd
from pathml.core import SlideDataset
from pathml.core.slide_data import VectraSlide
from pathml.core.slide_data import CODEXSlide
from pathml.preprocessing.pipeline import Pipeline
from pathml.preprocessing.transforms import SegmentMIF, QuantifyMIF, CollapseRunsCODEX
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import rc_context
from dask.distributed import Client, LocalCluster
from deepcell.utils.plot_utils import make_outline_overlay
from deepcell.utils.plot_utils import create_rgb_image
import scanpy as sc
import squidpy as sq
import anndata as ad
import bbknn
from joblib import parallel_backend

```

## Load Data

```{r}
channel_names <- readr::read_lines("data/CRC/channelNames.txt")

reticulate::use_condaenv("pathml-r", required = TRUE)
pml <- reticulate::import("pathml", convert = FALSE, delay_load = TRUE)

slide_dir <- "data/CRC/TMA_A"

slide_path <- fs::dir_ls(slide_dir)

slide_list <- slide_path |>
  purrr::map(
    .f = \(x) pml$core$slide_data$CODEXSlide(x, stain = "IF")
  )
attributes(slide_list)$names <- NULL

slide_dataset <- pml$core$SlideDataset(slide_list)

pipe_obj <- pml$preprocessing$pipeline$Pipeline(
  list(
    pml$preprocessing$transforms$CollapseRunsCODEX(
      z = 0L
    ),
    pml$preprocessing$transforms$SegmentMIF(
      model = "mesmer",
      nuclear_channel = 0L,
      cytoplasm_channel = 29L,
      image_resolution = 0.377442
    ),
    pml$preprocessing$transforms$QuantifyMIF(
      segmentation_mask = "cell_segmentation"
    )
  )
)

dask_dist <- reticulate::import("dask.distributed")

cluster_obj <- dask_dist$LocalCluster(
  n_workers = 10L,
  threads_per_worker = 1L,
  processes = TRUE
)
client_obj <- dask_dist$Client(cluster_obj)

pipeline_run <- pml$core$SlideDataset$run(
  self = slide_dataset,
  pipeline = pipe_obj,
  distributed = TRUE,
  client = client_obj,
  tile_size = reticulate::tuple(1920L, 1440L),
  tile_pad = FALSE
)
pml$core$SlideDataset$write(
  self = slide_dataset,
  "data/dataset_processed.h5"
)
```


### Merged call

```{r}
pml$core$SlideDataset$run(
  self = pml$core$SlideDataset(slide_list),
  pipeline = pml$preprocessing$pipeline$Pipeline(
    list(
      pml$preprocessing$transforms$CollapseRunsCODEX(
        z = 0L
      ),
      pml$preprocessing$transforms$SegmentMIF(
        model = "mesmer",
        nuclear_channel = 0L,
        cytoplasm_channel = 29L,
        image_resolution = 0.377442
      ),
      pml$preprocessing$transforms$QuantifyMIF(
        segmentation_mask = "cell_segmentation"
      )
    )
  ),
  distributed = TRUE,
  client = client_obj,
  tile_size = reticulate::tuple(1920L, 1440L),
  tile_pad = FALSE
)
```

### Python equivalente

```python
from os import listdir,path
import glob
import re

from pathml.core import SlideDataset
from pathml.core.slide_data import CODEXSlide
from pathml.preprocessing.pipeline import Pipeline
from pathml.preprocessing.transforms import SegmentMIF, QuantifyMIF, CollapseRunsCODEX

from dask.distributed import Client, LocalCluster



dirpath = r"data/CRC"

# assuming that all slides are in a single directory, all with .tif file extension
for A,B in [('TMA_A', 'TMA_B')]:
  vectra_list_A = [CODEXSlide(p, stain='IF') for p in glob.glob(path.join(dirpath, A, "*.tif"))]
  vectra_list_B = [CODEXSlide(p, stain='IF') for p in glob.glob(path.join(dirpath, B, "*.tif"))]

# Fix the slide names and add origin labels (A, B)
for slide_A, slide_B in zip(vectra_list_A, vectra_list_B):
  slide_A.name = re.sub("X.*", "A", slide_A.name)
  slide_B.name = re.sub("X.*", "B", slide_B.name)

# Store all slides in a SlideDataSet object
dataset = SlideDataset(vectra_list_A + vectra_list_B)


# Run Pipeline
pipe = Pipeline([
    CollapseRunsCODEX(z=0),
    SegmentMIF(model='mesmer', nuclear_channel=0, cytoplasm_channel=29, image_resolution=0.377442),
    QuantifyMIF(segmentation_mask='cell_segmentation')
])


# Initialize a dask cluster using 10 workers. PathML pipelines can be run in distributed mode on
# cloud compute or a cluster using dask.distributed.

cluster = LocalCluster(n_workers=10, threads_per_worker=1, processes=True)

client = Client(cluster)

# Run the pipeline

dataset.run(pipe, distributed = True, client = client, tile_size=(1920,1440), tile_pad=False)

dataset.write('data/dataset_processed.h5')

##  Combine the count matrices into a single adata object:
import anndata as ad
adata = ad.concat(
  [x.counts for x in dataset.slides],
  join="outer",
  label="Region",
  index_unique='_'
  )
# Fix and replace the regions names
origin = adata.obs['Region']
origin = origin.astype(str).str.replace("[^a-zA-Z0-9 \n\.]", "")
origin = origin.astype(str).str.replace("[\n]", "")
origin = origin.str.replace("SlideDataname", "")
adata.obs['Region'] = origin

# save the adata object
adata_combined.write(filename='./data/adata_combined.h5ad')

```
